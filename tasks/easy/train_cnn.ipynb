{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import pakietów","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nfrom sklearn.metrics import confusion_matrix","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-01T23:07:47.549890Z","iopub.execute_input":"2023-07-01T23:07:47.550316Z","iopub.status.idle":"2023-07-01T23:07:53.149127Z","shell.execute_reply.started":"2023-07-01T23:07:47.550281Z","shell.execute_reply":"2023-07-01T23:07:53.147957Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Wybieramy CPU lub GPU w zależności od dostępności.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tworzenie zbioru danych","metadata":{}},{"cell_type":"markdown","source":"Teraz rozdzielamy obrazy na validacyjne, testowe i treningowe.","metadata":{}},{"cell_type":"code","source":"!pip install split-folders[full]","metadata":{"execution":{"iopub.status.busy":"2023-07-01T23:13:35.774452Z","iopub.execute_input":"2023-07-01T23:13:35.774900Z","iopub.status.idle":"2023-07-01T23:13:50.473055Z","shell.execute_reply.started":"2023-07-01T23:13:35.774865Z","shell.execute_reply":"2023-07-01T23:13:50.471623Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting split-folders[full]\n  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from split-folders[full]) (4.64.1)\nInstalling collected packages: split-folders\nSuccessfully installed split-folders-0.5.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!ls /kaggle/input/five-flowers/original-jpegs","metadata":{"execution":{"iopub.status.busy":"2023-07-01T23:15:50.575599Z","iopub.execute_input":"2023-07-01T23:15:50.576416Z","iopub.status.idle":"2023-07-01T23:15:51.674598Z","shell.execute_reply.started":"2023-07-01T23:15:50.576362Z","shell.execute_reply":"2023-07-01T23:15:51.673106Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"daisy  dandelion  roses  sunflowers  tulips\n","output_type":"stream"}]},{"cell_type":"code","source":"!splitfolders --seed 42 --output /kaggle/working/data --ratio .7 .2 .1 -- /kaggle/input/five-flowers/original-jpegs","metadata":{"execution":{"iopub.status.busy":"2023-07-01T23:22:17.001550Z","iopub.execute_input":"2023-07-01T23:22:17.002031Z","iopub.status.idle":"2023-07-01T23:22:23.081883Z","shell.execute_reply.started":"2023-07-01T23:22:17.001989Z","shell.execute_reply":"2023-07-01T23:22:23.080622Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Copying files: 3670 files [00:04, 767.95 files/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"IMAGE_SIZE = (64, 64) # To wydaje sie optymalny rozmiar do tego problemu.\n\n# Transformacje\ntransformations = transforms.Compose([\n    # Tu wstaw transformacje\n    # Transformacje są dostępne na https://pytorch.org/vision/stable/transforms.html\n    # Przynajmniej potrzebujemy normalizacji wielkości losowej rotacji i flip. I zamiany na tensor (sklauje do <0, 1>)\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = \"/kaggle/working/data/train\"\nVAL_PATH = \"/kaggle/working/data/val\"\nTEST_PATH = \"/kaggle/working/data/test\"\n\n# Dataset do validacji i do trenowania\n# Stwórz z https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets.ImageFolder","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Teraz tworzymy obiekty do ładowania danych.","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"model = nn.Sequential([\n    # Tu wstaw warstwy modelu.\n    # Ekstraktor cech powinien mieć kilka bloków składających się z warstw konwolucyjnych, max pooling, ReLU i batch normalization.\n    # Na koniec potrzebujemy flatten aby zamienić obraz na wektor i potem liniowego klasyfikatora.\n    # Ilość wyjść to 5. Ilość wejść do warstwy liniowej to H*W*C wyjścia z ekstraktora cech. Można to ustalić eksperymentalnie poprzez wprowadzenie losowego wejścia do sieci.\n    # Zobacz torch.randn.\n    # Komponenty są dostępne na https://pytorch.org/docs/stable/nn.html\n])\n\nmodel = model.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optymalizator","metadata":{}},{"cell_type":"code","source":"LR = 1e-3 # Jeśli model nie będzie się trenował to pewnie trzeba to zmniejszyć.\nWEIGHT_DECAY = 0.0 # Waga do regularyzacji z L2\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Funkcja strat i pętal trenowania","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\nloss_train = []\nloss_val = []\n\nN_EPOCHS = 10 # Ile przejść przez zbiór danych.\nfor epoch in range(N_EPOCHS):\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n\n        preds = model(x)\n        loss = criterion(preds, y)\n\n        # Obliczanie gradientów\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loss_train.append(float(loss.cpu()))\n    \n    \n    ## Napisz kod do ewaluacji sieci na zbiorze valdacyjnym.\n    ## Oblicz średni loss i accuracy.\n    ## Predykcję można ustalić przy użyciu argmax w torch,","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot loss curves\nplt.plot(loss_train, label=\"train\")\nplt.plot(loss_val, label=\"val\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.legend()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ewaluacja ","metadata":{}},{"cell_type":"code","source":"model = model.eval() # Wyłącz aktualizację parametrów w normalizacji partii.\n## Napisz kod do ewaluacji modelu na zbiorze testowym. Oblicz accuracy. Ustal confusion matrix przy użyciu \n## https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n\n## Notatka: kod do predykcji powinien być w bloku with torch.no_grad(): żeby wyłączyć automatyczne różniczkowanie.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Eksport modelu do ONNX","metadata":{}},{"cell_type":"code","source":"dummy_input = torch.randn(1, 3, X, Y) # Ustaw rozmiar który był użyty na początku\ntorch.onnx.export(model, dummy_input, \"model.onnx\")","metadata":{},"execution_count":null,"outputs":[]}]}